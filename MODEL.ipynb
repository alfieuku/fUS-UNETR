{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c563093c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No files in fusiData/doppler matching **/*normcorre.mat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 365\u001b[0m\n\u001b[1;32m    361\u001b[0m OUT  \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfusi_splits_stream_80_10_10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    363\u001b[0m PATTERN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*normcorre.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# recursive; use \"*normcorre.mat\" if flat\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_and_cache_streaming_3way\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATTERN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# prefer session-level split across files\u001b[39;49;00m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# keep all variants of the same Sxxx_Ryy together\u001b[39;49;00m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mT_win\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_log1p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_zscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_short\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 224\u001b[0m, in \u001b[0;36mbuild_and_cache_streaming_3way\u001b[0;34m(root, out_dir, pattern, ratios, split_by, group_by, T_win, stride, apply_log1p, apply_zscore, pad_short, seed, chunk_size)\u001b[0m\n\u001b[1;32m    222\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(Path(root)\u001b[38;5;241m.\u001b[39mglob(pattern))\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m matching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    227\u001b[0m     (out_dir \u001b[38;5;241m/\u001b[39m tag)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No files in fusiData/doppler matching **/*normcorre.mat"
     ]
    }
   ],
   "source": [
    "# fUSI → train/val/test windows (80/10/10), streaming, grouped splits to prevent leakage\n",
    "# - Groups files by session (subject+run) or by subject before splitting\n",
    "# - Streams windows into small .pt chunks: out_dir/{train,val,test}/*\n",
    "# - Executes when you run the cell\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import json, gc, re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --------- deps ---------\n",
    "try:\n",
    "    import h5py\n",
    "except ImportError:\n",
    "    h5py = None\n",
    "try:\n",
    "    from scipy.io import loadmat\n",
    "except ImportError:\n",
    "    loadmat = None\n",
    "\n",
    "def _require_iolibs():\n",
    "    if h5py is None and loadmat is None:\n",
    "        raise RuntimeError(\n",
    "            \"Need at least one MAT loader. Install one of:\\n\"\n",
    "            \"  pip install h5py   # for v7.3 MAT\\n\"\n",
    "            \"  pip install scipy  # for legacy MAT\"\n",
    "        )\n",
    "\n",
    "# ----------------------- I/O helpers -----------------------\n",
    "def _find_candidate_key(h5) -> Optional[str]:\n",
    "    if h5py is None:\n",
    "        return None\n",
    "    hits = []\n",
    "    def visit(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            lname = name.lower()\n",
    "            if any(k in lname for k in [\"idop\", \"dop\", \"doppler\"]):\n",
    "                hits.append(name)\n",
    "    h5.visititems(visit)\n",
    "    for c in hits:\n",
    "        if c.split(\"/\")[-1].lower() == \"idop\":\n",
    "            return c\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def _load_mat_v73(path: Path) -> Optional[np.ndarray]:\n",
    "    if h5py is None:\n",
    "        return None\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            k = _find_candidate_key(f)\n",
    "            if k is None:\n",
    "                return None\n",
    "            return np.array(f[k])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _load_mat_legacy(path: Path) -> Optional[np.ndarray]:\n",
    "    if loadmat is None:\n",
    "        return None\n",
    "    try:\n",
    "        md = loadmat(path, squeeze_me=False, struct_as_record=False)\n",
    "        for k in [\"iDop\", \"IDop\", \"doppler\", \"Dop\", \"dop\"]:\n",
    "            if k in md:\n",
    "                return np.array(md[k])\n",
    "        for _, v in md.items():\n",
    "            if isinstance(v, np.ndarray) and v.ndim in (3, 4) and np.issubdtype(v.dtype, np.number):\n",
    "                return np.array(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def load_idop_any(path: Path) -> np.ndarray:\n",
    "    arr = _load_mat_v73(path)\n",
    "    if arr is None:\n",
    "        arr = _load_mat_legacy(path)\n",
    "    if arr is None:\n",
    "        raise ValueError(f\"Could not find Doppler array in {path}\")\n",
    "    return arr\n",
    "\n",
    "def canonicalize_idop(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return [N, T, H, W] (no channel).\n",
    "    3D: two largest -> spatial; remainder -> T.\n",
    "    4D: spatial = pair with minimal |a-b|; of remaining dims, smaller->T, larger->N.\n",
    "    \"\"\"\n",
    "    if arr.ndim == 3:\n",
    "        s = list(arr.shape)\n",
    "        idx = sorted(range(3), key=lambda i: s[i], reverse=True)\n",
    "        H, W, T = idx[0], idx[1], idx[2]\n",
    "        out = np.transpose(arr, (T, H, W))   # [T,H,W]\n",
    "        return out[None, ...].astype(np.float32, copy=False)  # [1,T,H,W]\n",
    "    if arr.ndim == 4:\n",
    "        s = list(arr.shape)\n",
    "        best_pair, best_diff = None, None\n",
    "        for i in range(4):\n",
    "            for j in range(i+1, 4):\n",
    "                d = abs(s[i] - s[j])\n",
    "                if best_diff is None or d < best_diff:\n",
    "                    best_diff, best_pair = d, (i, j)\n",
    "        H_idx, W_idx = best_pair\n",
    "        rem = [i for i in range(4) if i not in (H_idx, W_idx)]\n",
    "        T_idx, N_idx = (rem[0], rem[1]) if s[rem[0]] <= s[rem[1]] else (rem[1], rem[0])\n",
    "        out = np.transpose(arr, (N_idx, T_idx, H_idx, W_idx))  # [N,T,H,W]\n",
    "        return out.astype(np.float32, copy=False)\n",
    "    raise ValueError(f\"Unsupported ndim={arr.ndim}\")\n",
    "\n",
    "def log1p_safe(x: np.ndarray) -> np.ndarray:\n",
    "    if x.min() < 0:\n",
    "        x = x - x.min()\n",
    "    return np.log1p(x)\n",
    "\n",
    "def zscore_over_time_per_trial(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"x: [N,T,H,W] — z-score across time per pixel, per trial.\"\"\"\n",
    "    m = x.mean(axis=1, keepdims=True)\n",
    "    s = x.std(axis=1, keepdims=True)\n",
    "    return (x - m) / (s + eps)\n",
    "\n",
    "def make_windows(x_nthw: np.ndarray, T_win: int, stride: int, pad_short: bool = False) -> np.ndarray:\n",
    "    \"\"\"x: [N,T,H,W] → windows [M,T_win,1,H,W] (float32).\"\"\"\n",
    "    N, T, H, W = x_nthw.shape\n",
    "    out = []\n",
    "    for n in range(N):\n",
    "        if T < T_win:\n",
    "            if not pad_short:\n",
    "                continue\n",
    "            pad = T_win - T\n",
    "            pre, post = pad // 2, pad - pad // 2\n",
    "            x_t = np.pad(x_nthw[n], ((pre, post), (0, 0), (0, 0)), mode=\"edge\")\n",
    "            out.append(x_t[None, :, None, :, :])\n",
    "            continue\n",
    "        for t0 in range(0, T - T_win + 1, stride):\n",
    "            x_t = x_nthw[n, t0:t0 + T_win]\n",
    "            out.append(x_t[None, :, None, :, :])\n",
    "    if not out:\n",
    "        return np.empty((0, T_win, 1, H, W), dtype=np.float32)\n",
    "    return np.concatenate(out, axis=0).astype(np.float32, copy=False)\n",
    "\n",
    "# ----------------------- GROUPED splitting (3-way) -----------------------\n",
    "def _session_key(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Group files by (subject, run) if present; else by subject; else by stem.\n",
    "    Handles names like:\n",
    "      doppler_S129_R1+normcorre.mat\n",
    "      doppler_S129_R1_allTrials+normcorre.mat\n",
    "      dopplerContinuous_S129_R1+normcorre.mat\n",
    "    \"\"\"\n",
    "    name = p.name\n",
    "    m = re.search(r'_S(\\d+)_R(\\d+)', name, re.IGNORECASE)\n",
    "    if m:\n",
    "        return f\"S{m.group(1)}_R{m.group(2)}\"\n",
    "    m = re.search(r'_S(\\d+)', name, re.IGNORECASE)\n",
    "    if m:\n",
    "        return f\"S{m.group(1)}\"\n",
    "    return p.stem\n",
    "\n",
    "def _subject_key(p: Path) -> str:\n",
    "    name = p.name\n",
    "    m = re.search(r'_S(\\d+)', name, re.IGNORECASE)\n",
    "    return f\"S{m.group(1)}\" if m else p.stem\n",
    "\n",
    "def split_files_3way_grouped(\n",
    "    files: List[Path],\n",
    "    ratios=(0.8, 0.1, 0.1),\n",
    "    seed: int = 0,\n",
    "    group_by: str = \"session\"  # \"session\" (subject+run) or \"subject\"\n",
    ") -> Tuple[List[Path], List[Path], List[Path]]:\n",
    "    assert abs(sum(ratios) - 1.0) < 1e-6, \"ratios must sum to 1.0\"\n",
    "    key_fn = _session_key if group_by == \"session\" else _subject_key\n",
    "\n",
    "    # build groups\n",
    "    groups: Dict[str, List[Path]] = {}\n",
    "    for f in files:\n",
    "        k = key_fn(f)\n",
    "        groups.setdefault(k, []).append(f)\n",
    "\n",
    "    # shuffle group ids\n",
    "    rng = np.random.RandomState(seed)\n",
    "    gids = list(groups.keys())\n",
    "    rng.shuffle(gids)\n",
    "\n",
    "    n = len(gids)\n",
    "    n_tr = int(np.floor(ratios[0]*n))\n",
    "    n_va = int(np.floor(ratios[1]*n))\n",
    "    n_te = n - n_tr - n_va\n",
    "    if n >= 2 and n_te == 0:\n",
    "        if n_va > 0: n_va -= 1; n_te = 1\n",
    "        elif n_tr > 1: n_tr -= 1; n_te = 1\n",
    "\n",
    "    tr_ids = set(gids[:n_tr])\n",
    "    va_ids = set(gids[n_tr:n_tr+n_va])\n",
    "    te_ids = set(gids[n_tr+n_va:])\n",
    "\n",
    "    train = [f for gid in tr_ids for f in groups[gid]]\n",
    "    val   = [f for gid in va_ids for f in groups[gid]]\n",
    "    test  = [f for gid in te_ids for f in groups[gid]]\n",
    "    return train, val, test\n",
    "\n",
    "# --------------- STREAMING builder (writes small chunks) ---------------\n",
    "def build_and_cache_streaming_3way(\n",
    "    root: Path,\n",
    "    out_dir: Path,\n",
    "    pattern: str = \"*normcorre.mat\",\n",
    "    ratios=(0.8, 0.1, 0.1),\n",
    "    split_by: str = \"file\",           # \"file\" or \"trial\"\n",
    "    group_by: str = \"session\",        # only used when split_by=\"file\"\n",
    "    T_win: int = 8,\n",
    "    stride: int = 4,\n",
    "    apply_log1p: bool = True,\n",
    "    apply_zscore: bool = True,\n",
    "    pad_short: bool = False,\n",
    "    seed: int = 0,\n",
    "    chunk_size: int = 32,             # small to avoid OOM in notebooks\n",
    ") -> Dict[str, List[Path]]:\n",
    "    \"\"\"\n",
    "    Streams windows into out_dir/{train,val,test}/ as small .pt files.\n",
    "    Splits by grouped files to prevent leakage across variants.\n",
    "    Returns dict listing the chunk file paths in each split.\n",
    "    \"\"\"\n",
    "    _require_iolibs()\n",
    "    out_dir = Path(out_dir)\n",
    "    files = sorted(Path(root).glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files in {root} matching {pattern}\")\n",
    "\n",
    "    for tag in (\"train\", \"val\", \"test\"):\n",
    "        (out_dir / tag).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    saved = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    totals = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "    def _save_chunks(tag: str, wins: np.ndarray, stem: str, counter: int) -> int:\n",
    "        if wins.size == 0:\n",
    "            return counter\n",
    "        n = wins.shape[0]\n",
    "        for i in range(0, n, chunk_size):\n",
    "            part = torch.from_numpy(wins[i:i + chunk_size].copy())\n",
    "            path = out_dir / tag / f\"{stem}_part{counter:05d}.pt\"\n",
    "            torch.save(part, path)\n",
    "            saved[tag].append(path)\n",
    "            counter += 1\n",
    "        totals[tag] += n\n",
    "        return counter\n",
    "\n",
    "    if split_by == \"file\" and len(files) > 1:\n",
    "        train_files, val_files, test_files = split_files_3way_grouped(\n",
    "            files, ratios=ratios, seed=seed, group_by=group_by\n",
    "        )\n",
    "        sets = [(\"train\", train_files), (\"val\", val_files), (\"test\", test_files)]\n",
    "        counters = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        for tag, fset in sets:\n",
    "            for f in fset:\n",
    "                try:\n",
    "                    arr = load_idop_any(f)\n",
    "                    x = canonicalize_idop(arr)  # [N,T,H,W]\n",
    "                    if apply_log1p: x = log1p_safe(x)\n",
    "                    if apply_zscore: x = zscore_over_time_per_trial(x)\n",
    "                    wins = make_windows(x, T_win=T_win, stride=stride, pad_short=pad_short)\n",
    "                    counters[tag] = _save_chunks(tag, wins, f.stem, counters[tag])\n",
    "                    print(f\"[{tag}] {f.name}: trials={x.shape[0]} T={x.shape[1]} → saved {wins.shape[0]} windows\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[{tag}] SKIP {f.name}: {e}\")\n",
    "                finally:\n",
    "                    del arr, x\n",
    "                    if 'wins' in locals(): del wins\n",
    "                    gc.collect()\n",
    "    else:\n",
    "        # Single-file or explicit trial split (3-way inside each file)\n",
    "        counters = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        for f in files:\n",
    "            try:\n",
    "                arr = load_idop_any(f)\n",
    "                x = canonicalize_idop(arr)\n",
    "                if apply_log1p: x = log1p_safe(x)\n",
    "                if apply_zscore: x = zscore_over_time_per_trial(x)\n",
    "                # 3-way split of trials\n",
    "                rng = np.random.RandomState(seed)\n",
    "                idx = np.arange(x.shape[0]); rng.shuffle(idx)\n",
    "                n = x.shape[0]\n",
    "                n_tr = int(np.floor(ratios[0]*n))\n",
    "                n_va = int(np.floor(ratios[1]*n))\n",
    "                tr_idx = np.sort(idx[:n_tr])\n",
    "                va_idx = np.sort(idx[n_tr:n_tr+n_va])\n",
    "                te_idx = np.sort(idx[n_tr+n_va:])\n",
    "                w_tr = make_windows(x[tr_idx], T_win=T_win, stride=stride, pad_short=pad_short)\n",
    "                w_va = make_windows(x[va_idx], T_win=T_win, stride=stride, pad_short=pad_short)\n",
    "                w_te = make_windows(x[te_idx], T_win=T_win, stride=stride, pad_short=pad_short)\n",
    "                counters[\"train\"] = _save_chunks(\"train\", w_tr, f.stem, counters[\"train\"])\n",
    "                counters[\"val\"]   = _save_chunks(\"val\",   w_va, f.stem, counters[\"val\"])\n",
    "                counters[\"test\"]  = _save_chunks(\"test\",  w_te, f.stem, counters[\"test\"])\n",
    "                print(f\"[trial-3way] {f.name}: train {w_tr.shape[0]} | val {w_va.shape[0]} | test {w_te.shape[0]} windows\")\n",
    "            except Exception as e:\n",
    "                print(f\"[trial-3way] SKIP {f.name}: {e}\")\n",
    "            finally:\n",
    "                del arr, x\n",
    "                if 'w_tr' in locals(): del w_tr\n",
    "                if 'w_va' in locals(): del w_va\n",
    "                if 'w_te' in locals(): del w_te\n",
    "                gc.collect()\n",
    "\n",
    "    # Write manifest\n",
    "    manifest = {\n",
    "        \"root\": str(root),\n",
    "        \"pattern\": pattern,\n",
    "        \"ratios\": ratios,\n",
    "        \"split_by\": split_by,\n",
    "        \"group_by\": group_by,\n",
    "        \"T_win\": T_win,\n",
    "        \"stride\": stride,\n",
    "        \"apply_log1p\": apply_log1p,\n",
    "        \"apply_zscore\": apply_zscore,\n",
    "        \"pad_short\": pad_short,\n",
    "        \"seed\": seed,\n",
    "        \"counts\": totals,\n",
    "        \"train_files\": [str(p) for p in saved[\"train\"]],\n",
    "        \"val_files\":   [str(p) for p in saved[\"val\"]],\n",
    "        \"test_files\":  [str(p) for p in saved[\"test\"]],\n",
    "    }\n",
    "    with open(out_dir / \"manifest.json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved chunks → train: {totals['train']} windows in {len(saved['train'])} files, \"\n",
    "          f\"val: {totals['val']} in {len(saved['val'])}, test: {totals['test']} in {len(saved['test'])}.\")\n",
    "    print(f\"Manifest: {out_dir/'manifest.json'}\")\n",
    "    return saved\n",
    "\n",
    "# ---------------- Lazy dataset for the saved chunks ----------------\n",
    "class FUSIChunkedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Loads window chunks lazily from out_dir/train, val, or test.\"\"\"\n",
    "    def __init__(self, chunk_paths: List[Path]):\n",
    "        self.files = [Path(p) for p in chunk_paths]\n",
    "        self._sizes = []\n",
    "        for p in self.files:\n",
    "            t = torch.load(p, map_location=\"cpu\")\n",
    "            self._sizes.append(int(t.shape[0]))\n",
    "        self._cum = np.cumsum([0] + self._sizes)\n",
    "        self._cache_idx = None\n",
    "        self._cache_tensor = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(self._cum[-1])\n",
    "\n",
    "    def _locate(self, idx: int) -> Tuple[int, int]:\n",
    "        c = int(np.searchsorted(self._cum, idx, side=\"right\") - 1)\n",
    "        off = idx - self._cum[c]\n",
    "        return c, off\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        c, off = self._locate(idx)\n",
    "        if self._cache_idx != c:\n",
    "            self._cache_tensor = torch.load(self.files[c], map_location=\"cpu\")\n",
    "            self._cache_idx = c\n",
    "        return self._cache_tensor[off]\n",
    "\n",
    "def load_manifest(manifest_path: Path) -> Dict:\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ------------------------- RUN (executes now) -------------------------\n",
    "ROOT = Path(\"fusiData/doppler\")\n",
    "OUT  = Path(\"fusi_splits_stream_80_10_10\")\n",
    "\n",
    "PATTERN = \"**/*normcorre.mat\"  # recursive; use \"*normcorre.mat\" if flat\n",
    "\n",
    "_ = build_and_cache_streaming_3way(\n",
    "    root=ROOT,\n",
    "    out_dir=OUT,\n",
    "    pattern=PATTERN,\n",
    "    ratios=(0.8, 0.1, 0.1),\n",
    "    split_by=\"file\",          # prefer session-level split across files\n",
    "    group_by=\"session\",       # keep all variants of the same Sxxx_Ryy together\n",
    "    T_win=8,\n",
    "    stride=4,\n",
    "    apply_log1p=True,\n",
    "    apply_zscore=True,\n",
    "    pad_short=False,\n",
    "    seed=0,\n",
    "    chunk_size=32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99d73e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_unet_temporal.py\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---- UNet building blocks ----\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):  # [B,C,H,W]\n",
    "        return self.net(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, 2, stride=2)\n",
    "        self.conv = DoubleConv(in_ch // 2 + out_ch, out_ch)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        dh, dw = skip.size(2) - x.size(2), skip.size(3) - x.size(3)\n",
    "        if dh or dw:\n",
    "            if dh < 0: x = x[:, :, :skip.size(2), :]\n",
    "            if dw < 0: x = x[:, :, :, :skip.size(3)]\n",
    "            dh, dw = skip.size(2) - x.size(2), skip.size(3) - x.size(3)\n",
    "            if dh > 0 or dw > 0:\n",
    "                x = F.pad(x, (0, max(dw,0), 0, max(dh,0)))\n",
    "        return self.conv(torch.cat([x, skip], dim=1))\n",
    "\n",
    "# ---- temporal transformer over T only ----\n",
    "class SinusoidalPosEnc(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos*div), torch.cos(pos*div)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "    def forward(self, x):  # [B,T,d]\n",
    "        return x + self.pe[:x.size(1)].unsqueeze(0).to(x.dtype)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, nlayers=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                         dim_feedforward=4*d_model, dropout=dropout,\n",
    "                                         batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=nlayers)\n",
    "        self.pos = SinusoidalPosEnc(d_model)\n",
    "    def forward(self, seq):  # [B,T,d]\n",
    "        return self.encoder(self.pos(seq))\n",
    "\n",
    "# ---- FiLM conditioning (temporal -> spatial decoder) ----\n",
    "class FiLMHead(nn.Module):\n",
    "    def __init__(self, d_model: int, channels: int):\n",
    "        super().__init__()\n",
    "        hidden = max(128, d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, hidden), nn.SiLU(),\n",
    "                                 nn.Linear(hidden, 2*channels))\n",
    "        self.C = channels\n",
    "    def forward(self, z):  # [B,T,d]\n",
    "        gb = self.mlp(z)   # [B,T,2C]\n",
    "        return gb[..., :self.C], gb[..., self.C:]  # gamma, beta\n",
    "\n",
    "# ---- full model ----\n",
    "class UNetTemporalDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Input/Output: x ∈ R[B, T, 1, H, W]  →  y ∈ R[B, T, 1, H, W]\n",
    "    Spatial UNet runs per-frame; temporal transformer runs along T.\n",
    "    Transformer outputs modulate decoder via FiLM per time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, base=32, d_model=128, nhead=4, nlayers=3,\n",
    "                 dropout=0.0, predict_residual=True):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4 = base, 2*base, 4*base, 8*base\n",
    "        # encoder\n",
    "        self.inc   = DoubleConv(1, c1)\n",
    "        self.down1 = Down(c1, c2)\n",
    "        self.down2 = Down(c2, c3)\n",
    "        self.down3 = Down(c3, c4)\n",
    "        # tokens from multi-scale pooled features\n",
    "        self.to_token = nn.Linear(c2 + c3 + c4, d_model)\n",
    "        self.temporal = TemporalTransformer(d_model=d_model, nhead=nhead, nlayers=nlayers, dropout=dropout)\n",
    "        # decoder + FiLM\n",
    "        self.up3  = Up(c4, c3); self.film3 = FiLMHead(d_model, c3)\n",
    "        self.up2  = Up(c3, c2); self.film2 = FiLMHead(d_model, c2)\n",
    "        self.up1  = Up(c2, c1); self.film1 = FiLMHead(d_model, c1)\n",
    "        self.outc = nn.Conv2d(c1, 1, 1)\n",
    "        self.predict_residual = predict_residual\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _film(x_bt, g_bt, b_bt):\n",
    "        return x_bt * (1.0 + g_bt).unsqueeze(-1).unsqueeze(-1) + b_bt.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    def forward(self, x):  # x: [B,T,1,H,W]\n",
    "        B, T, _, H, W = x.shape\n",
    "        x_bt = x.view(B*T, 1, H, W)\n",
    "\n",
    "        # encoder per frame\n",
    "        e1 = self.inc(x_bt)        # [B*T,c1,H,W]\n",
    "        e2 = self.down1(e1)        # [B*T,c2,H/2,W/2]\n",
    "        e3 = self.down2(e2)        # [B*T,c3,H/4,W/4]\n",
    "        b  = self.down3(e3)        # [B*T,c4,H/8,W/8]\n",
    "\n",
    "        # pooled tokens -> transformer over T\n",
    "        p2 = F.adaptive_avg_pool2d(e2, 1).flatten(1)\n",
    "        p3 = F.adaptive_avg_pool2d(e3, 1).flatten(1)\n",
    "        p4 = F.adaptive_avg_pool2d(b,  1).flatten(1)\n",
    "        tok = torch.cat([p2, p3, p4], dim=1)          # [B*T, c2+c3+c4]\n",
    "        z = self.temporal(self.to_token(tok).view(B, T, -1))  # [B,T,d_model]\n",
    "\n",
    "        # FiLM params per time → flatten to [B*T,C]\n",
    "        def bt(tC): return tC.reshape(B*T, -1)\n",
    "        g3, b3 = bt(self.film3(z)[0]), bt(self.film3(z)[1])\n",
    "        g2, b2 = bt(self.film2(z)[0]), bt(self.film2(z)[1])\n",
    "        g1, b1 = bt(self.film1(z)[0]), bt(self.film1(z)[1])\n",
    "\n",
    "        # decoder per frame + FiLM\n",
    "        d3 = self._film(self.up3(b, e3), g3, b3)\n",
    "        d2 = self._film(self.up2(d3, e2), g2, b2)\n",
    "        d1 = self._film(self.up1(d2, e1), g1, b1)\n",
    "        y_bt = self.outc(d1)                       # [B*T,1,H,W]\n",
    "        y = y_bt.view(B, T, 1, H, W)\n",
    "        return x + y if self.predict_residual else y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77d8f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps | AMP: False (None)  |  phys-noise: True\n",
      "  step 10: train loss 2.53553\n",
      "  step 20: train loss 1.02374\n",
      "  step 30: train loss 1.12471\n",
      "  step 40: train loss 0.85075\n",
      "  step 50: train loss 0.82841\n",
      "Epoch 01 | 130.8s train: loss 2.75089, PSNR 3.80 | val: loss 0.93624, PSNR 5.47\n",
      "  step 10: train loss 1.15655\n",
      "  step 20: train loss 0.90731\n",
      "  step 30: train loss 0.14757\n",
      "  step 40: train loss 0.95641\n",
      "  step 50: train loss 0.91374\n",
      "Epoch 02 | 132.6s train: loss 0.80090, PSNR 6.88 | val: loss 0.77935, PSNR 5.79\n",
      "  step 10: train loss 0.79201\n",
      "  step 20: train loss 1.00765\n",
      "  step 30: train loss 0.81595\n",
      "  step 40: train loss 0.82681\n",
      "  step 50: train loss 0.96873\n",
      "Epoch 03 | 862.7s train: loss 0.70940, PSNR 7.30 | val: loss 0.71869, PSNR 6.02\n",
      "  step 10: train loss 0.21448\n",
      "  step 20: train loss 0.65280\n",
      "  step 30: train loss 0.83225\n",
      "  step 40: train loss 0.73815\n",
      "  step 50: train loss 0.79972\n",
      "Epoch 04 | 2715.5s train: loss 0.62269, PSNR 7.90 | val: loss 0.67268, PSNR 6.23\n",
      "  step 10: train loss 0.57199\n",
      "  step 20: train loss 0.69865\n",
      "  step 30: train loss 0.70600\n",
      "  step 40: train loss 0.86747\n",
      "  step 50: train loss 0.86979\n",
      "Epoch 05 | 140.2s train: loss 0.64674, PSNR 7.32 | val: loss 0.72446, PSNR 6.13\n",
      "Done. Best val loss: 0.672675  |  wrote weights in: fusi_splits_stream_80_10_10/weights\n"
     ]
    }
   ],
   "source": [
    "# train_fusi_denoiser.py\n",
    "# -- MPS-safe, OOM-resistant training with realistic fUSI-like noise --\n",
    "import os, math, bisect, json, time, random, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional: helps MPS avoid watermark OOMs\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "# ---------------- Device & AMP ----------------\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "DEVICE = pick_device()\n",
    "USE_AMP = (DEVICE == \"cuda\")\n",
    "AMP_DTYPE = torch.float16 if DEVICE == \"cuda\" else None\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# ---------------- Model (inline) ----------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, 2, stride=2)\n",
    "        self.conv = DoubleConv(in_ch // 2 + out_ch, out_ch)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        dh, dw = skip.size(2) - x.size(2), skip.size(3) - x.size(3)\n",
    "        if dh or dw:\n",
    "            if dh < 0: x = x[:, :, :skip.size(2), :]\n",
    "            if dw < 0: x = x[:, :, :, :skip.size(3)]\n",
    "            dh, dw = skip.size(2) - x.size(2), skip.size(3) - x.size(3)\n",
    "            if dh > 0 or dw > 0:\n",
    "                x = F.pad(x, (0, max(dw,0), 0, max(dh,0)))\n",
    "        return self.conv(torch.cat([x, skip], dim=1))\n",
    "\n",
    "class SinusoidalPosEnc(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2], pe[:, 1::2] = torch.sin(pos*div), torch.cos(pos*div)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "    def forward(self, x):  # [B,T,d]\n",
    "        return x + self.pe[:x.size(1)].unsqueeze(0).to(x.dtype)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, nlayers=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                         dim_feedforward=4*d_model, dropout=dropout,\n",
    "                                         batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=nlayers)\n",
    "        self.pos = SinusoidalPosEnc(d_model)\n",
    "    def forward(self, seq):  # [B,T,d]\n",
    "        return self.encoder(self.pos(seq))\n",
    "\n",
    "class FiLMHead(nn.Module):\n",
    "    def __init__(self, d_model: int, channels: int):\n",
    "        super().__init__()\n",
    "        hidden = max(128, d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, hidden), nn.SiLU(),\n",
    "                                 nn.Linear(hidden, 2*channels))\n",
    "        self.C = channels\n",
    "    def forward(self, z):  # [B,T,d]\n",
    "        gb = self.mlp(z)   # [B,T,2C]\n",
    "        return gb[..., :self.C], gb[..., self.C:]  # gamma, beta\n",
    "\n",
    "class UNetTemporalDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Input/Output: x ∈ R[B, T, 1, H, W]  →  y ∈ R[B, T, 1, H, W]\n",
    "    Spatial UNet runs per-frame; temporal transformer runs along T.\n",
    "    \"\"\"\n",
    "    def __init__(self, base=32, d_model=128, nhead=4, nlayers=3,\n",
    "                 dropout=0.0, predict_residual=True):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4 = base, 2*base, 4*base, 8*base\n",
    "        # encoder\n",
    "        self.inc   = DoubleConv(1, c1)\n",
    "        self.down1 = Down(c1, c2)\n",
    "        self.down2 = Down(c2, c3)\n",
    "        self.down3 = Down(c3, c4)\n",
    "        # tokens from multi-scale pooled features\n",
    "        self.to_token = nn.Linear(c2 + c3 + c4, d_model)\n",
    "        self.temporal = TemporalTransformer(d_model=d_model, nhead=nhead, nlayers=nlayers, dropout=dropout)\n",
    "        # decoder + FiLM\n",
    "        self.up3  = Up(c4, c3); self.film3 = FiLMHead(d_model, c3)\n",
    "        self.up2  = Up(c3, c2); self.film2 = FiLMHead(d_model, c2)\n",
    "        self.up1  = Up(c2, c1); self.film1 = FiLMHead(d_model, c1)\n",
    "        self.outc = nn.Conv2d(c1, 1, 1)\n",
    "        self.predict_residual = predict_residual\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _film(x_bt, g_bt, b_bt):\n",
    "        return x_bt * (1.0 + g_bt).unsqueeze(-1).unsqueeze(-1) + b_bt.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    def forward(self, x):  # x: [B,T,1,H,W]\n",
    "        B, T, _, H, W = x.shape\n",
    "        x_bt = x.view(B*T, 1, H, W)\n",
    "\n",
    "        # encoder per frame\n",
    "        e1 = self.inc(x_bt)\n",
    "        e2 = self.down1(e1)\n",
    "        e3 = self.down2(e2)\n",
    "        b  = self.down3(e3)\n",
    "\n",
    "        # pooled tokens -> transformer over T\n",
    "        p2 = F.adaptive_avg_pool2d(e2, 1).flatten(1)\n",
    "        p3 = F.adaptive_avg_pool2d(e3, 1).flatten(1)\n",
    "        p4 = F.adaptive_avg_pool2d(b,  1).flatten(1)\n",
    "        tok = torch.cat([p2, p3, p4], dim=1)          # [B*T, c2+c3+c4]\n",
    "        z = self.temporal(self.to_token(tok).view(B, T, -1))  # [B,T,d_model]\n",
    "\n",
    "        # FiLM params per time → flatten to [B*T,C]\n",
    "        def bt(tC): return tC.reshape(B*T, -1)\n",
    "        g3, b3 = bt(self.film3(z)[0]), bt(self.film3(z)[1])\n",
    "        g2, b2 = bt(self.film2(z)[0]), bt(self.film2(z)[1])\n",
    "        g1, b1 = bt(self.film1(z)[0]), bt(self.film1(z)[1])\n",
    "\n",
    "        # decoder per frame + FiLM\n",
    "        d3 = self._film(self.up3(b, e3), g3, b3)\n",
    "        d2 = self._film(self.up2(d3, e2), g2, b2)\n",
    "        d1 = self._film(self.up1(d2, e1), g1, b1)\n",
    "        y_bt = self.outc(d1)\n",
    "        y = y_bt.view(B, T, 1, H, W)\n",
    "        return x + y if self.predict_residual else y\n",
    "\n",
    "# ---------------- Dataset over saved shards ----------------\n",
    "class FUSIChunkedDataset(Dataset):\n",
    "    \"\"\"Each item is a clip [T,1,H,W] loaded on demand from shard .pt files.\"\"\"\n",
    "    def __init__(self, shard_paths: List[Path]):\n",
    "        self.files = [Path(p) for p in shard_paths]\n",
    "        self._sizes = []\n",
    "        for p in self.files:\n",
    "            t = torch.load(p, map_location=\"cpu\")  # [M,T,1,H,W]\n",
    "            self._sizes.append(int(t.shape[0]))\n",
    "        self._cum: List[int] = [0]\n",
    "        for n in self._sizes:\n",
    "            self._cum.append(self._cum[-1] + n)\n",
    "        self._cache_idx = None\n",
    "        self._cache = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._cum[-1]\n",
    "\n",
    "    def _locate(self, idx: int) -> Tuple[int, int]:\n",
    "        i = bisect.bisect_right(self._cum, int(idx)) - 1\n",
    "        i = max(0, min(i, len(self.files) - 1))\n",
    "        off = int(idx) - self._cum[i]\n",
    "        return i, off\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(idx)\n",
    "        shard, off = self._locate(idx)\n",
    "        if self._cache_idx != shard:\n",
    "            self._cache = torch.load(self.files[shard], map_location=\"cpu\")  # [M,T,1,H,W]\n",
    "            self._cache_idx = shard\n",
    "        return self._cache[off].float()  # [T,1,H,W]\n",
    "\n",
    "# ---------------- Collate: pad/crop ----------------\n",
    "def _center_crop_or_pad(x: torch.Tensor, Ht: int, Wt: int) -> torch.Tensor:\n",
    "    _, _, H, W = x.shape\n",
    "    if H > Ht:\n",
    "        top = (H - Ht) // 2\n",
    "        x = x[:, :, top:top + Ht, :]\n",
    "    if W > Wt:\n",
    "        left = (W - Wt) // 2\n",
    "        x = x[:, :, :, left:left + Wt]\n",
    "    pad_h, pad_w = Ht - x.shape[2], Wt - x.shape[3]\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, pad_w, 0, pad_h), mode=\"replicate\")\n",
    "    return x\n",
    "\n",
    "def collate_and_pad(batch, max_hw: Optional[Tuple[int, int]] = None):\n",
    "    Tset = {b.shape[0] for b in batch}\n",
    "    if len(Tset) != 1:\n",
    "        raise ValueError(f\"Mixed T in batch: {Tset}\")\n",
    "    if max_hw is None:\n",
    "        Ht = max(b.shape[2] for b in batch)\n",
    "        Wt = max(b.shape[3] for b in batch)\n",
    "    else:\n",
    "        Ht, Wt = max_hw\n",
    "    out = [_center_crop_or_pad(x, Ht, Wt) for x in batch]\n",
    "    return torch.stack(out, dim=0)  # [B,T,1,Ht,Wt]\n",
    "\n",
    "# ---------------- Manifest helpers (auto-build if missing) ----------------\n",
    "def _build_manifest_from_shards(split_dir: Path) -> dict:\n",
    "    shards = sorted(str(p) for p in split_dir.glob(\"*.pt\"))\n",
    "    if not shards:\n",
    "        shards = sorted(str(p) for p in split_dir.rglob(\"*.pt\"))\n",
    "    if not shards:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No manifest.json and no .pt shards found under {split_dir}.\\n\"\n",
    "            f\"Set SPLIT_DIR to your folder with saved shards.\"\n",
    "        )\n",
    "    n = len(shards)\n",
    "    n_train = max(1, int(round(0.8 * n)))\n",
    "    n_val   = max(1, int(round(0.1 * n)))\n",
    "    n_test  = max(1, n - n_train - n_val)\n",
    "    if n_train + n_val + n_test > n:\n",
    "        n_test = n - n_train - n_val\n",
    "        if n_test < 1:\n",
    "            n_test = 1\n",
    "            n_val = max(1, n - n_train - n_test)\n",
    "            if n_train + n_val + n_test > n:\n",
    "                n_train = n - n_val - n_test\n",
    "\n",
    "    man = {\n",
    "        \"train_files\": shards[:n_train],\n",
    "        \"val_files\":   shards[n_train:n_train+n_val] if n_val > 0 else shards[:1],\n",
    "        \"test_files\":  shards[n_train+n_val:] if n_test > 0 else shards[-1:],\n",
    "        \"note\": \"Auto-generated manifest (80/10/10) because manifest.json was missing.\"\n",
    "    }\n",
    "    with open(split_dir / \"manifest.json\", \"w\") as f:\n",
    "        json.dump(man, f, indent=2)\n",
    "    print(f\"[autosplit] Built manifest.json with {n_train}/{n_val}/{n_test} shards in {split_dir}\")\n",
    "    return man\n",
    "\n",
    "def _load_manifest(split_dir: Path) -> dict:\n",
    "    mpath = split_dir / \"manifest.json\"\n",
    "    if mpath.exists():\n",
    "        with open(mpath, \"r\") as f:\n",
    "            man = json.load(f)\n",
    "        for k in (\"train_files\", \"val_files\", \"test_files\"):\n",
    "            if k not in man or not isinstance(man[k], list) or len(man[k]) == 0:\n",
    "                raise ValueError(f\"manifest.json missing or empty '{k}'.\")\n",
    "        return man\n",
    "    else:\n",
    "        return _build_manifest_from_shards(split_dir)\n",
    "\n",
    "def make_loaders(split_dir: Path, batch_size=1, num_workers=0, max_hw: Optional[Tuple[int, int]] = None):\n",
    "    split_dir = Path(os.path.expanduser(str(split_dir))).resolve()\n",
    "    if not split_dir.exists():\n",
    "        raise FileNotFoundError(f\"SPLIT_DIR does not exist: {split_dir}\")\n",
    "    man = _load_manifest(split_dir)\n",
    "\n",
    "    train_ds = FUSIChunkedDataset([Path(p) for p in man[\"train_files\"]])\n",
    "    val_ds   = FUSIChunkedDataset([Path(p) for p in man[\"val_files\"]])\n",
    "    test_ds  = FUSIChunkedDataset([Path(p) for p in man[\"test_files\"]])\n",
    "\n",
    "    cf = (lambda b: collate_and_pad(b, max_hw=max_hw))\n",
    "    pin = (DEVICE == \"cuda\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin, persistent_workers=False,\n",
    "                              collate_fn=cf)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin, persistent_workers=False,\n",
    "                              collate_fn=cf)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin, persistent_workers=False,\n",
    "                              collate_fn=cf)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ---------------- Metrics & noise ----------------\n",
    "def temporal_tv(x):\n",
    "    return (x[:, 1:] - x[:, :-1]).abs().mean()\n",
    "\n",
    "def psnr(x, y, eps=1e-8):\n",
    "    mse = F.mse_loss(x, y)\n",
    "    return 10.0 * torch.log10(1.0 / (mse + eps))\n",
    "\n",
    "def add_noise(x, sigma=0.05, relative=True):\n",
    "    if relative:\n",
    "        s = x.std(dim=(2, 3, 4), keepdim=True).clamp_min(1e-6)\n",
    "        noise = torch.randn_like(x) * (sigma * s)\n",
    "    else:\n",
    "        noise = torch.randn_like(x) * sigma\n",
    "    return x + noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def _integer_jitter(x: torch.Tensor, jitter_px: int) -> torch.Tensor:\n",
    "    \"\"\"Per-frame replicate-pad jitter. x: [B,T,1,H,W]\"\"\"\n",
    "    if jitter_px <= 0:\n",
    "        return x\n",
    "    B, T, C, H, W = x.shape\n",
    "    out = torch.empty_like(x)\n",
    "    for b in range(B):\n",
    "        dx = torch.randint(-jitter_px, jitter_px + 1, (T,), device=x.device)\n",
    "        dy = torch.randint(-jitter_px, jitter_px + 1, (T,), device=x.device)\n",
    "        for t in range(T):\n",
    "            frame = x[b, t].unsqueeze(0)  # [1,1,H,W]\n",
    "            pad = F.pad(frame, (jitter_px, jitter_px, jitter_px, jitter_px), mode=\"replicate\")\n",
    "            x0 = jitter_px + int(dx[t].item())\n",
    "            y0 = jitter_px + int(dy[t].item())\n",
    "            out[b, t] = pad[:, :, y0:y0+H, x0:x0+W]\n",
    "    return out\n",
    "\n",
    "def add_phys_like_noise(\n",
    "    x: torch.Tensor,               # [B,T,1,H,W]\n",
    "    fps: float = 10.0,\n",
    "    sigma_white: float = 0.02,\n",
    "    resp_band: Tuple[float,float] = (0.2, 0.4),\n",
    "    card_band: Tuple[float,float] = (0.8, 1.2),\n",
    "    lowrank_scale: float = 0.2,\n",
    "    jitter_px: int = 1,\n",
    ") -> torch.Tensor:\n",
    "    B,T,_,H,W = x.shape\n",
    "    device = x.device\n",
    "    t = torch.arange(T, device=device).float() / max(fps, 1e-6)\n",
    "    fr = torch.empty(B, device=device).uniform_(*resp_band)\n",
    "    fc = torch.empty(B, device=device).uniform_(*card_band)\n",
    "    phir = torch.empty(B, device=device).uniform_(0, 2*math.pi)\n",
    "    phic = torch.empty(B, device=device).uniform_(0, 2*math.pi)\n",
    "    resp = torch.sin(2*math.pi*fr[:,None]*t + phir[:,None])\n",
    "    card = torch.sin(2*math.pi*fc[:,None]*t + phic[:,None])\n",
    "    phys = (resp + 0.5*card).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # [B,T,1,1,1]\n",
    "\n",
    "    # smooth spatial map -> broadcast over T\n",
    "    Hs, Ws = max(1, H//8), max(1, W//8)\n",
    "    spatial_lr = torch.randn(B, 1, Hs, Ws, device=device)\n",
    "    spatial = F.interpolate(spatial_lr, size=(H, W), mode=\"bilinear\", align_corners=False)  # (B,1,H,W)\n",
    "    spatial = spatial / (spatial.flatten(1).std(dim=1, keepdim=True).clamp_min(1e-6)).view(B,1,1,1)\n",
    "    spatial_bt = spatial.unsqueeze(1).expand(B, T, 1, H, W)\n",
    "\n",
    "    lowrank = lowrank_scale * phys * spatial_bt\n",
    "    x_jit = _integer_jitter(x, jitter_px=jitter_px)\n",
    "    s = x.std(dim=(2,3,4), keepdim=True).clamp_min(1e-6)\n",
    "    white = torch.randn_like(x) * (sigma_white * s)\n",
    "    return x_jit + lowrank + white\n",
    "\n",
    "# ---------------- Safe weights-only save helpers ----------------\n",
    "def _safe_save(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    torch.save(obj, tmp)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def _snapshot_weights(model: nn.Module, epoch: int, step: int, val_loss: Optional[float], out_dir: Path, tag: str):\n",
    "    payload = {\n",
    "        \"model\": model.state_dict(),        # weights only\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": step,\n",
    "        \"val_loss\": float(val_loss) if val_loss is not None else None,\n",
    "        \"timestamp\": time.time(),\n",
    "    }\n",
    "    _safe_save(payload, out_dir / f\"{tag}.pt\")\n",
    "\n",
    "# ---------------- Training loop (hourly + step saves, best/last/final) ----------------\n",
    "def train_fusi(\n",
    "    split_dir: Path,\n",
    "    epochs=1,\n",
    "    batch_size=1,\n",
    "    lr=2e-4,\n",
    "    weight_decay=1e-5,\n",
    "    # noise\n",
    "    use_phys_noise: bool = True,\n",
    "    fps: float = 10.0,\n",
    "    noise_sigma: float = 0.05,\n",
    "    sigma_white: float = 0.02,\n",
    "    resp_band: Tuple[float,float] = (0.2, 0.4),\n",
    "    card_band: Tuple[float,float] = (0.8, 1.2),\n",
    "    lowrank_scale: float = 0.2,\n",
    "    jitter_px: int = 1,\n",
    "    # reg\n",
    "    lambda_tv: float = 0.05,\n",
    "    # model size\n",
    "    base_channels=12,\n",
    "    d_model=64,\n",
    "    nlayers=1,\n",
    "    # memory controls\n",
    "    max_hw: Optional[Tuple[int, int]] = (128, 160),\n",
    "    train_patch_hw: Optional[Tuple[int, int]] = (112, 144),\n",
    "    # logging & save cadence\n",
    "    log_every: int = 50,\n",
    "    empty_cache_every: int = 10,\n",
    "    save_every_seconds: int = 3600,      # hourly saves\n",
    "    save_every_steps: int = 0,           # e.g., 200 to also save by step\n",
    "    max_steps_per_epoch: Optional[int] = None,  # limit steps for quick runs\n",
    "    # optional model class\n",
    "    model_cls: Optional[Type[nn.Module]] = None,\n",
    "):\n",
    "    print(f\"Device: {DEVICE} | AMP: {USE_AMP} ({AMP_DTYPE})  |  phys-noise: {use_phys_noise}\")\n",
    "    torch.backends.cudnn.benchmark = (DEVICE == \"cuda\")\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader, val_loader, _ = make_loaders(split_dir, batch_size=batch_size, num_workers=0, max_hw=max_hw)\n",
    "\n",
    "    # Model\n",
    "    if model_cls is None:\n",
    "        model_cls = UNetTemporalDenoiser\n",
    "    model = model_cls(base=base_channels, d_model=d_model, nhead=4, nlayers=nlayers, predict_residual=True).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scaler = torch.amp.GradScaler(device=\"cuda\") if DEVICE == \"cuda\" else None\n",
    "\n",
    "    weights_dir = Path(split_dir) / \"weights\"   # weights-only folder\n",
    "    weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    # initial save\n",
    "    _snapshot_weights(model, epoch=0, step=0, val_loss=None, out_dir=weights_dir, tag=\"init\")\n",
    "\n",
    "    ctx_device = \"cuda\" if DEVICE == \"cuda\" else (\"mps\" if DEVICE == \"mps\" else \"cpu\")\n",
    "    def make_noisy(xb: torch.Tensor, train: bool) -> torch.Tensor:\n",
    "        if use_phys_noise:\n",
    "            return add_phys_like_noise(\n",
    "                xb, fps=fps, sigma_white=sigma_white,\n",
    "                resp_band=resp_band, card_band=card_band,\n",
    "                lowrank_scale=lowrank_scale, jitter_px=jitter_px\n",
    "            )\n",
    "        else:\n",
    "            return add_noise(xb, sigma=noise_sigma, relative=True)\n",
    "\n",
    "    global_step = 0\n",
    "    last_save_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            t0 = time.time()\n",
    "            # ----- train -----\n",
    "            model.train()\n",
    "            tr_loss = tr_psnr = 0.0\n",
    "            n_seen = 0\n",
    "            step = 0\n",
    "            for xb in train_loader:\n",
    "                step += 1\n",
    "                global_step += 1\n",
    "                xb = xb.float()\n",
    "\n",
    "                # optional random crop on CPU for memory\n",
    "                if train_patch_hw is not None:\n",
    "                    B, T, C, H, W = xb.shape\n",
    "                    Hc, Wc = min(train_patch_hw[0], H), min(train_patch_hw[1], W)\n",
    "                    if (Hc < H) or (Wc < W):\n",
    "                        top = random.randint(0, H - Hc)\n",
    "                        left = random.randint(0, W - Wc)\n",
    "                        xb = xb[:, :, :, top:top + Hc, left:left + Wc]\n",
    "\n",
    "                xb = xb.to(DEVICE, non_blocking=(DEVICE == \"cuda\"))\n",
    "                with torch.no_grad():\n",
    "                    x_noisy = make_noisy(xb, train=True)\n",
    "\n",
    "                if USE_AMP:\n",
    "                    with torch.amp.autocast(device_type=ctx_device, dtype=AMP_DTYPE):\n",
    "                        y = model(x_noisy)\n",
    "                        loss = F.mse_loss(y, xb) + lambda_tv * temporal_tv(y)\n",
    "                else:\n",
    "                    y = model(x_noisy)\n",
    "                    loss = F.mse_loss(y, xb) + lambda_tv * temporal_tv(y)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                if DEVICE == \"cuda\":\n",
    "                    scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(opt); scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    opt.step()\n",
    "\n",
    "                bs = xb.size(0)\n",
    "                n_seen += bs\n",
    "                tr_loss += loss.item() * bs\n",
    "                with torch.no_grad():\n",
    "                    tr_psnr += psnr(y.clamp_min(0), xb.clamp_min(0)).item() * bs\n",
    "\n",
    "                if (step % log_every) == 0:\n",
    "                    print(f\"  step {step}: train loss {loss.item():.5f}\")\n",
    "\n",
    "                # Step-based save (optional)\n",
    "                if save_every_steps and (global_step % save_every_steps == 0):\n",
    "                    _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=None, out_dir=weights_dir, tag=f\"step-{global_step}\")\n",
    "                    _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=None, out_dir=weights_dir, tag=\"last\")\n",
    "\n",
    "                # Hourly weights save\n",
    "                now = time.time()\n",
    "                if save_every_seconds and (now - last_save_time) >= save_every_seconds:\n",
    "                    stamp = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=None, out_dir=weights_dir, tag=f\"hourly-{stamp}\")\n",
    "                    _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=None, out_dir=weights_dir, tag=\"last\")\n",
    "                    last_save_time = now\n",
    "\n",
    "                if DEVICE == \"mps\" and (step % empty_cache_every) == 0:\n",
    "                    del x_noisy, y, loss\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "                # Optional cap for quick runs\n",
    "                if (max_steps_per_epoch is not None) and (step >= max_steps_per_epoch):\n",
    "                    break\n",
    "\n",
    "            tr_loss /= max(1, n_seen)\n",
    "            tr_psnr /= max(1, n_seen)\n",
    "\n",
    "            # ----- val -----\n",
    "            model.eval()\n",
    "            va_loss = va_psnr = 0.0\n",
    "            n_seen = 0\n",
    "            with torch.no_grad():\n",
    "                for xb in val_loader:\n",
    "                    xb = xb.float().to(DEVICE)\n",
    "                    x_noisy = make_noisy(xb, train=False)\n",
    "                    if USE_AMP:\n",
    "                        with torch.amp.autocast(device_type=ctx_device, dtype=AMP_DTYPE):\n",
    "                            y = model(x_noisy)\n",
    "                            loss = F.mse_loss(y, xb) + lambda_tv * temporal_tv(y)\n",
    "                    else:\n",
    "                        y = model(x_noisy)\n",
    "                        loss = F.mse_loss(y, xb) + lambda_tv * temporal_tv(y)\n",
    "                    bs = xb.size(0)\n",
    "                    n_seen += bs\n",
    "                    va_loss += loss.item() * bs\n",
    "                    va_psnr += psnr(y.clamp_min(0), xb.clamp_min(0)).item() * bs\n",
    "                    if DEVICE == \"mps\":\n",
    "                        del x_noisy, y, loss\n",
    "                        torch.mps.empty_cache()\n",
    "\n",
    "            va_loss = (va_loss / n_seen) if n_seen > 0 else float(\"inf\")\n",
    "            va_psnr = (va_psnr / n_seen) if n_seen > 0 else float(\"nan\")\n",
    "\n",
    "            if DEVICE == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "            dt_epoch = time.time() - t0\n",
    "            print(f\"Epoch {epoch:02d} | {dt_epoch:5.1f}s \"\n",
    "                  f\"train: loss {tr_loss:.5f}, PSNR {tr_psnr:.2f} | \"\n",
    "                  f\"val: loss {va_loss:.5f}, PSNR {va_psnr:.2f}\")\n",
    "\n",
    "            # Save last & best (weights only)\n",
    "            _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=va_loss, out_dir=weights_dir, tag=\"last\")\n",
    "            if math.isfinite(va_loss) and (va_loss < best_val):\n",
    "                best_val = va_loss\n",
    "                _snapshot_weights(model, epoch=epoch, step=global_step, val_loss=va_loss, out_dir=weights_dir, tag=\"best\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboardInterrupt caught — saving interrupt weights...\")\n",
    "        _snapshot_weights(model, epoch=locals().get('epoch', 0), step=locals().get('global_step', 0),\n",
    "                          val_loss=None, out_dir=weights_dir, tag=\"interrupt\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Always write a final weights file\n",
    "        _snapshot_weights(model, epoch=epochs, step=global_step,\n",
    "                          val_loss=best_val if math.isfinite(best_val) else None,\n",
    "                          out_dir=weights_dir, tag=\"final\")\n",
    "        print(f\"Done. Best val loss: {best_val:.6f}  |  wrote weights in: {weights_dir}\")\n",
    "    return model\n",
    "\n",
    "# ---- runner ----\n",
    "if __name__ == \"__main__\":\n",
    "    # set this to your real split path\n",
    "    SPLIT_DIR = Path(\"fusi_splits_stream_80_10_10\")\n",
    "    _ = train_fusi(\n",
    "        split_dir=SPLIT_DIR,\n",
    "        epochs=5,            # quick pass\n",
    "        batch_size=1,\n",
    "        lr=2e-4,\n",
    "        use_phys_noise=True,\n",
    "        fps=10.0,\n",
    "        sigma_white=0.02,\n",
    "        resp_band=(0.2, 0.4),    # human; for rodent try (1.0, 3.0)\n",
    "        card_band=(0.8, 1.2),    # human; for rodent try (6.0, 10.0)\n",
    "        lowrank_scale=0.2,\n",
    "        jitter_px=1,\n",
    "        lambda_tv=0.05,\n",
    "        base_channels=12,\n",
    "        d_model=64,\n",
    "        nlayers=1,\n",
    "        max_hw=(128, 160),\n",
    "        train_patch_hw=(112, 144),\n",
    "        save_every_seconds=3600,  # hourly weights\n",
    "        save_every_steps=0,       # set e.g. 200 to also save by step\n",
    "        max_steps_per_epoch=50,   # cap for speed; raise/remove for full training\n",
    "        log_every=10,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03464e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume] inferred → base=12, d_model=64, nlayers=1, nhead=4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_fusi() got an unexpected keyword argument 'init_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m base, d_model, nlayers, nhead \u001b[38;5;241m=\u001b[39m infer_hparams_from_weights(WEIGHTS)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[resume] inferred → base=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, d_model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nlayers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnlayers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nhead=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fusi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPLIT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# resume from best.pt (make sure train_fusi supports init_weights)\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# lower LR for resume\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_phys_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_white\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp_band\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcard_band\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowrank_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjitter_px\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_tv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use inferred architecture\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_hw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_patch_hw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# switch to (112,144) if you hit OOM\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mempty_cache_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_fusi() got an unexpected keyword argument 'init_weights'"
     ]
    }
   ],
   "source": [
    "# runner_resume_best.py (PyTorch, Python 3.9-safe)\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def infer_hparams_from_weights(weights_path: Union[str, Path]) -> Tuple[int, int, int, int]:\n",
    "    pkg = torch.load(weights_path, map_location=\"cpu\")\n",
    "    state = pkg.get(\"model\", pkg)\n",
    "\n",
    "    base = state[\"inc.net.0.weight\"].shape[0]            # first conv out-ch\n",
    "    d_model = state[\"to_token.weight\"].shape[0]          # rows of to_token\n",
    "    # count transformer layers\n",
    "    layer_idxs = []\n",
    "    pat = re.compile(r\"^temporal\\.encoder\\.layers\\.(\\d+)\\.\")\n",
    "    for k in state.keys():\n",
    "        m = pat.match(k)\n",
    "        if m:\n",
    "            layer_idxs.append(int(m.group(1)))\n",
    "    nlayers = (max(layer_idxs) + 1) if layer_idxs else 1\n",
    "    nhead = 4  # can't infer from weights; matches your training\n",
    "\n",
    "    return base, d_model, nlayers, nhead\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    WEIGHTS = Path(\"fusi_splits_stream_80_10_10/weights/best.pt\")\n",
    "    SPLIT_DIR = Path(\"fusi_splits_stream_80_10_10\")\n",
    "\n",
    "    base, d_model, nlayers, nhead = infer_hparams_from_weights(WEIGHTS)\n",
    "    print(f\"[resume] inferred → base={base}, d_model={d_model}, nlayers={nlayers}, nhead={nhead}\")\n",
    "\n",
    "    _ = train_fusi(\n",
    "        split_dir=SPLIT_DIR,\n",
    "        # resume from best.pt (make sure train_fusi supports init_weights)\n",
    "        init_weights=WEIGHTS,\n",
    "        epochs=20,\n",
    "        batch_size=1,\n",
    "        lr=1e-4,                   # lower LR for resume\n",
    "        weight_decay=1e-5,\n",
    "\n",
    "        use_phys_noise=True,\n",
    "        fps=10.0,\n",
    "        sigma_white=0.02,\n",
    "        resp_band=(0.2, 0.4),\n",
    "        card_band=(0.8, 1.2),\n",
    "        lowrank_scale=0.2,\n",
    "        jitter_px=1,\n",
    "        lambda_tv=0.05,\n",
    "\n",
    "        # use inferred architecture\n",
    "        base_channels=base,\n",
    "        d_model=d_model,\n",
    "        nlayers=nlayers,\n",
    "\n",
    "        max_hw=(128, 160),\n",
    "        train_patch_hw=None,       # switch to (112,144) if you hit OOM\n",
    "\n",
    "        save_every_seconds=3600,\n",
    "        log_every=50,\n",
    "        empty_cache_every=10,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77980f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on: mps | meta: {'epoch': 1, 'step': 621, 'val_loss': None, 'timestamp': 1754973217.4593768}\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Dict, Any\n",
    "import torch\n",
    "\n",
    "def pick_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "def load_model_from_weights(\n",
    "    weights_path: Union[str, Path],\n",
    "    *,\n",
    "    base: int = 12,\n",
    "    d_model: int = 64,\n",
    "    nhead: int = 4,\n",
    "    nlayers: int = 1,\n",
    "    predict_residual: bool = True,\n",
    "    device: Optional[str] = None,\n",
    ") -> tuple[UNetTemporalDenoiser, str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a model from a weights-only .pt file produced by your training script.\n",
    "    Pass the same hyperparams used during training (base, d_model, nhead, nlayers, predict_residual).\n",
    "    Returns (model, device, meta_dict).\n",
    "    \"\"\"\n",
    "    device = device or pick_device()\n",
    "    weights_path = Path(weights_path)\n",
    "\n",
    "    if not weights_path.exists():\n",
    "        raise FileNotFoundError(f\"Can't find weights file: {weights_path}\")\n",
    "\n",
    "    pkg = torch.load(weights_path, map_location=device)\n",
    "\n",
    "    # Accept either {\"model\": state_dict, ...} or plain state_dict\n",
    "    if isinstance(pkg, dict) and \"model\" in pkg:\n",
    "        state = pkg[\"model\"]\n",
    "        meta = {k: pkg.get(k) for k in (\"epoch\", \"step\", \"val_loss\", \"timestamp\")}\n",
    "        # If a config was saved, prefer it unless caller overrides\n",
    "        cfg = pkg.get(\"config\") or {}\n",
    "        base = cfg.get(\"base\", base)\n",
    "        d_model = cfg.get(\"d_model\", d_model)\n",
    "        nhead = cfg.get(\"nhead\", nhead)\n",
    "        nlayers = cfg.get(\"nlayers\", nlayers)\n",
    "        if \"predict_residual\" in cfg:\n",
    "            predict_residual = cfg[\"predict_residual\"]\n",
    "    else:\n",
    "        state = pkg\n",
    "        meta = {}\n",
    "\n",
    "    model = UNetTemporalDenoiser(\n",
    "        base=base, d_model=d_model, nhead=nhead, nlayers=nlayers,\n",
    "        predict_residual=predict_residual\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(state, strict=True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Strict load failed; trying non-strict.\\nDetails:\\n\", e)\n",
    "        missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "        print(\"Loaded with strict=False.\\nMissing keys:\", missing, \"\\nUnexpected keys:\", unexpected)\n",
    "\n",
    "    model.eval()\n",
    "    return model, device, meta\n",
    "\n",
    "\n",
    "# ---- example usage ----\n",
    "if __name__ == \"__main__\":\n",
    "    weights_file = \"fusi_splits_stream_80_10_10/weights/final.pt\"\n",
    "    model, device, meta = load_model_from_weights(\n",
    "        weights_file,\n",
    "        base=12, d_model=64, nhead=4, nlayers=1, predict_residual=True\n",
    "    )\n",
    "    print(\"Loaded on:\", device, \"| meta:\", meta)\n",
    "\n",
    "    # Optional quick test\n",
    "    # x = torch.randn(1, 16, 1, 112, 144, device=device)\n",
    "    # with torch.no_grad():\n",
    "    #     y = model(x)\n",
    "    # print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b62198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4870ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
